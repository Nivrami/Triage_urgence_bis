"""
Page Streamlit  - Chatbot + ML Predictor + Monitoring
"""

import streamlit as st
from pathlib import Path
import sys

sys.path.insert(0, str(Path(__file__).resolve().parents[2]))

from src.rag.chatbot import TriageChatbotAPI
from src.rag.predictor import MLTriagePredictor
from src.rag.entry_forms import render_entry_forms

# Config
st.set_page_config(page_title="Chatbot Triage ML", page_icon="ğŸ¥", layout="wide")

st.title("ğŸ¥ Chatbot de Triage des Urgences")
st.markdown("*Assistant ML pour aide Ã  la dÃ©cision*")

 # --- Formulaires "IdentitÃ©" et "Constantes" ---
render_entry_forms()
st.divider()

# Session
if "chatbot" not in st.session_state:
    # Charger RAG
    retriever = None
    with st.spinner("ğŸ”„ Chargement RAG..."):
        try:
            project_root = Path(__file__).resolve().parents[2]
            vector_db_path = project_root / "data" / "vector_db"

            from src.rag.vector_store import VectorStore, RAGRetriever

            vector_store = VectorStore(
                persist_directory=str(vector_db_path), collection_name="triage_medical"
            )
            retriever = RAGRetriever(vector_store=vector_store)

            st.session_state.predictor = MLTriagePredictor(rag_retriever=retriever)
            st.success("âœ… RAG chargÃ© avec succÃ¨s")
        except Exception as e:
            st.warning(f"âš ï¸ RAG non chargÃ©: {e}")
            st.session_state.predictor = MLTriagePredictor()

    # CrÃ©er chatbot AVEC le retriever RAG
    st.session_state.chatbot = TriageChatbotAPI(retriever=retriever)
    st.session_state.messages = []
    st.session_state.started = False
    st.session_state.prediction = None

bot = st.session_state.chatbot
predictor = st.session_state.predictor
data = bot.data

# Sidebar
with st.sidebar:
    st.header("ğŸ§‘â€âš•ï¸ Dossier patient")

    st.subheader("IdentitÃ©")
    st.write(f"**NÂ° patient:** {data.get('num_patient') or 'â€”'}")
    st.write(f"**Ã‚ge:** {data.get('age') or 'â€”'}")
    sex = "Homme" if data.get("genre") == "Homme" else "Femme" if data.get("genre") == "Femme" else "â€”"
    st.write(f"**Genre:** {sex}")
    st.divider()

    st.subheader("SymptÃ´mes")
    if data["symptoms"]:
        for s in data["symptoms"]:
            st.write(f"â€¢ {s}")
    else:
        st.write("â€”")
    st.divider()

    st.subheader("Constantes vitales")
    v = data["vitals"]
    count = len([k for k in ["Temperature", "FC", "TA_systolique", "SpO2", "FR"] if k in v])
    st.write(f"**Progression: {count}/5**")
    if v:
        if "Temperature" in v:
            st.write(f"ğŸŒ¡ï¸ Temp: {v['temp']}Â°C")
        if "FC" in v:
            st.write(f"â¤ï¸ FC: {v['fc']} bpm")
        if "TA_systolique" in v:
            st.write(f"ğŸ’‰ TA: {v['tas']}/{v.get('tad', '?')}")
        if "SpO2" in v:
            st.write(f"ğŸ« SpO2: {v['spo2']}%")
        if "FR" in v:
            st.write(f"ğŸŒ¬ï¸ FR: {v['fr']}/min")
    else:
        st.write("â€”")
    st.divider()

    # Actions
    if st.button("ğŸ”„ Nouvelle conversation", use_container_width=True):
        bot.reset()
        st.session_state.messages = []
        st.session_state.started = False
        st.session_state.prediction = None
        st.rerun()

    ready = bot.is_ready_for_prediction()

    if st.button(
        "ğŸ¯ Obtenir prÃ©diction ML",
        use_container_width=True,
        disabled=not ready,
        help="5/5 constantes requises",
    ):
        with st.spinner("ğŸ”® Analyse ML en cours..."):
            summary = bot.get_summary()
            st.session_state.prediction = predictor.predict(summary)
        st.rerun()

# Conversation
col1, col2 = st.columns([2, 1])

with col1:
    st.subheader("ğŸ’¬ Conversation")

    if not st.session_state.started:
        if st.button("ğŸš€ DÃ©marrer", use_container_width=True):
            msg = bot.start()
            st.session_state.messages.append({"role": "assistant", "content": msg})
            st.session_state.started = True
            st.rerun()

    # Messages
    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    # Input
    if st.session_state.started and not st.session_state.prediction:
        user_input = st.chat_input("Votre message...")
        if user_input:
            st.session_state.messages.append({"role": "user", "content": user_input})
            response = bot.chat(user_input)
            st.session_state.messages.append({"role": "assistant", "content": response})
            st.rerun()

# PrÃ©diction
with col2:
    st.subheader("ğŸ¯ RÃ©sultat ML")

    if st.session_state.prediction:
        r = st.session_state.prediction

        # Badge niveau
        st.markdown(
            f"""
        <div style="
            background-color:{r['color']};
            padding:20px;
            border-radius:10px;
            text-align:center;
            font-size:22px;
            font-weight:bold;
            color:{'white' if r['severity_level'] in ['ROUGE','GRIS'] else 'black'};
            margin-bottom:20px;
        ">{r['label']}</div>
        """,
            unsafe_allow_html=True,
        )

        st.info(f"**Action:** {r['action']}")

        # Drapeaux rouges
        if r.get("red_flags"):
            st.warning("âš ï¸ **Signes de gravitÃ©:**")
            for f in r["red_flags"]:
                st.markdown(f"- {f}")

        # ProbabilitÃ©s
        st.subheader("ğŸ“Š ProbabilitÃ©s ML")
        for lvl in ["ROUGE", "JAUNE", "VERT", "GRIS"]:
            p = r["probabilities"].get(lvl, 0)
            st.progress(p, text=f"{lvl}: {p*100:.1f}%")

        st.metric("Confiance", f"{r['confidence']*100:.1f}%")

        # Features
        if r.get("features_used"):
            with st.expander("ğŸ” Features ML"):
                st.json(r["features_used"])

        # RAG Sources
        if r.get("rag_sources"):
            with st.expander("ğŸ“š Sources RAG"):
                for source in r["rag_sources"]:
                    st.markdown(f"- {source}")

        # Justification
        with st.expander("ğŸ“‹ Justification ML + RAG", expanded=True):
            st.markdown(r["justification"])

        # Export
        st.divider()
        if st.button("ğŸ’¾ Exporter rapport", use_container_width=True):
            export = f"# RAPPORT TRIAGE ML\n\n"
            export += f"## {r['label']}\n"
            export += f"**Action:** {r['action']}\n"
            export += f"**Confiance:** {r['confidence']*100:.1f}%\n\n"

            if r.get("red_flags"):
                export += "## Drapeaux rouges\n"
                for f in r["red_flags"]:
                    export += f"- {f}\n"

            export += "\n## Conversation\n\n"
            for m in st.session_state.messages:
                role = "Assistant" if m["role"] == "assistant" else "Patient"
                export += f"**{role}:** {m['content']}\n\n"

            st.download_button(
                "ğŸ“¥ TÃ©lÃ©charger",
                export,
                "rapport_triage_ml.md",
                "text/markdown",
                use_container_width=True,
            )
    else:
        st.info(
            "ğŸ’¡ ComplÃ©tez la conversation (5/5 constantes) puis cliquez 'Obtenir prÃ©diction ML'"
        )

        with st.expander("â“ Aide"):
            st.markdown(
                """
            **Constantes requises (5/5):**
            - âœ… TempÃ©rature
            - âœ… FrÃ©quence cardiaque (FC)
            - âœ… Tension artÃ©rielle (TA)
            - âœ… Saturation oxygÃ¨ne (SpO2)
            - âœ… FrÃ©quence respiratoire (FR)

            Le bot vous guidera Ã©tape par Ã©tape.
            """
            )

st.divider()
st.caption(
    "ğŸ¤– ML (Random Forest) + ğŸ“š RAG (Documents mÃ©dicaux) + ğŸ”® Mistral API | âš ï¸ Outil d'aide - ne remplace pas avis mÃ©dical"
)
