"""
Page 1 : G√©n√©ration de conversations automatiques.
"""

import streamlit as st
import json
import sys
from pathlib import Path

# Ajouter le chemin src au PYTHONPATH
root_path = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_path))

from src.llm.llm_factory import LLMFactory
from src.workflows.simulation_workflow import SimulationWorkflow

# Configuration
st.set_page_config(
    page_title="G√©n√©ration - Triage Urgences",
    page_icon="üé≤",
    layout="wide"
)

# Titre
st.title("üé≤ G√©n√©ration de Conversations")

st.markdown("""
G√©n√©rez des conversations automatiques entre un infirmier et un patient pour cr√©er un dataset de triage.
""")

# Sidebar - Param√®tres
with st.sidebar:
    st.header("‚öôÔ∏è Param√®tres")
    
    max_turns = st.slider(
        "Nombre max de questions",
        min_value=3,
        max_value=15,
        value=8,
        help="Nombre maximum de questions que l'infirmier peut poser"
    )
    
    st.markdown("---")
    
    st.markdown("""
    ### üí° Astuce
    
    Laissez la pathologie vide pour une g√©n√©ration al√©atoire !
    """)

# Initialisation session state
if 'conversations' not in st.session_state:
    st.session_state.conversations = []

if 'current_result' not in st.session_state:
    st.session_state.current_result = None

# Zone principale
col1, col2 = st.columns([2, 1])

with col1:
    st.subheader("üìù Configuration")
    
    # Input pathologie
    pathology_input = st.text_input(
        "Pathologie (optionnel)",
        placeholder="Ex: Homme de 65 ans avec infarctus",
        help="Laissez vide pour une g√©n√©ration al√©atoire"
    )

with col2:
    st.subheader("üöÄ Actions")
    
    # Boutons
    if st.button("üé≤ G√©n√©rer 1 conversation", type="primary"):
        with st.spinner("G√©n√©ration en cours..."):
            try:
                # Initialiser LLM
                llm = LLMFactory.create("mistral", "mistral-large-latest")
                workflow = SimulationWorkflow(llm, max_turns=max_turns)
                
                # G√©n√©rer
                pathology = pathology_input if pathology_input.strip() else None
                
                # Capturer stdout pour afficher logs
                import io
                from contextlib import redirect_stdout
                import time
                
                log_stream = io.StringIO()
                start_time = time.time()
                
                with redirect_stdout(log_stream):
                    result = workflow.run_simulation(pathology=pathology)
                
                duration = time.time() - start_time
                
                # Track g√©n√©ration
                try:
                    sys.path.insert(0, str(root_path / "src"))
                    from src.monitoring.metrics_tracker import get_tracker
                    
                    tracker = get_tracker()
                    
                    # Track latence g√©n√©ration
                    tracker.track_latency(
                        component="Generation",
                        operation="conversation",
                        duration=duration
                    )
                    
                    # Track appel API LLM (estimation)
                    # Approximation: ~500 tokens input, ~300 tokens output par conversation
                    tracker.track_api_call(
                        service="mistral",
                        model="mistral-large-latest",
                        tokens_input=500,
                        tokens_output=300,
                        latency=duration,
                        success=True
                    )
                except Exception as track_error:
                    print(f"Monitoring error: {track_error}")
                
                # Sauvegarder
                st.session_state.current_result = result
                st.session_state.conversations.append(workflow.export_for_ml())
                
                st.success(f"‚úÖ Conversation g√©n√©r√©e en {duration:.2f}s!")
                st.rerun()
                
            except Exception as e:
                st.error(f"‚ùå Erreur : {str(e)}")
    
    if st.button("üìä G√©n√©rer 10 conversations"):
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        try:
            llm = LLMFactory.create("mistral", "mistral-large-latest")
            workflow = SimulationWorkflow(llm, max_turns=max_turns)
            
            import time
            total_duration = 0
            
            for i in range(10):
                status_text.text(f"G√©n√©ration {i+1}/10...")
                progress_bar.progress((i + 1) / 10)
                
                import io
                from contextlib import redirect_stdout
                log_stream = io.StringIO()
                
                start_time = time.time()
                
                with redirect_stdout(log_stream):
                    result = workflow.run_simulation()
                
                duration = time.time() - start_time
                total_duration += duration
                
                st.session_state.conversations.append(workflow.export_for_ml())
                workflow.reset()
                
                # Track chaque g√©n√©ration
                try:
                    sys.path.insert(0, str(root_path / "src"))
                    from src.monitoring.metrics_tracker import get_tracker  
                    
                    tracker = get_tracker()
                    tracker.track_latency("Generation", "conversation", duration)
                    tracker.track_api_call(
                        service="mistral",
                        model="mistral-large-latest",
                        tokens_input=500,
                        tokens_output=300,
                        latency=duration,
                        success=True
                    )
                except:
                    pass
            
            st.success(f"‚úÖ 10 conversations g√©n√©r√©es en {total_duration:.1f}s!")
            progress_bar.empty()
            status_text.empty()
            st.rerun()
            
        except Exception as e:
            st.error(f"‚ùå Erreur : {str(e)}")

# Affichage r√©sultat
st.markdown("---")

if st.session_state.current_result:
    result = st.session_state.current_result
    
    st.subheader("üìã Derni√®re conversation g√©n√©r√©e")
    
    # Pathologie
    st.info(f"**Pathologie :** {result['pathology']}")
    
    # Extraire les donn√©es pour utilisation
    extr = result['extracted_patient']
    orig = result['original_patient']
    
    # ‚≠ê NOUVELLE SECTION : G√©n√©ration des constantes
    st.markdown("---")
    st.markdown("### ü©∫ Mesure des Constantes Vitales")
    
    with st.expander("‚ÑπÔ∏è Comment sont g√©n√©r√©es les constantes ?", expanded=False):
        st.markdown("""
        **Processus automatique :**
        
        1. üß† L'IA analyse la pathologie simul√©e
        2. üìä G√©n√®re des constantes **coh√©rentes** avec cette pathologie
        3. ‚úÖ L'infirmier "mesure" ces valeurs avec ses appareils
        
        **Exemple :** Si la pathologie est "infarctus", les constantes refl√©teront :
        - FC √©lev√©e (stress cardiaque)
        - SpO2 basse (probl√®me oxyg√©nation)
        - TA anormale, etc.
        """)
    
    st.markdown("**L'infirmier proc√®de maintenant aux mesures avec ses appareils :**")
    
    # Afficher les constantes avec des badges color√©s
    if orig.constantes:
        c = orig.constantes
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # FC
            fc_status = "üü¢" if 60 <= c.fc <= 100 else "üü°" if 50 <= c.fc <= 120 else "üî¥"
            fc_note = "Normale" if 60 <= c.fc <= 100 else "L√©g√®rement √©lev√©e" if c.fc > 100 else "Basse"
            st.metric("üíì Fr√©quence Cardiaque (FC)", f"{c.fc} bpm", help=f"{fc_status} {fc_note}")
            
            # FR
            fr_status = "üü¢" if 12 <= c.fr <= 20 else "üü°" if 10 <= c.fr <= 25 else "üî¥"
            fr_note = "Normale" if 12 <= c.fr <= 20 else "√âlev√©e" if c.fr > 20 else "Basse"
            st.metric("ü´Å Fr√©quence Respiratoire (FR)", f"{c.fr} /min", help=f"{fr_status} {fr_note}")
        
        with col2:
            # SpO2
            spo2_status = "üü¢" if c.spo2 >= 95 else "üü°" if c.spo2 >= 90 else "üî¥"
            spo2_note = "Normale" if c.spo2 >= 95 else "Basse - Hypoxie" if c.spo2 < 90 else "L√©g√®rement basse"
            st.metric("ü©∏ Saturation Oxyg√®ne (SpO2)", f"{c.spo2}%", help=f"{spo2_status} {spo2_note}")
            
            # Temp√©rature
            temp_status = "üü¢" if 36.5 <= c.temperature <= 37.5 else "üü°" if c.temperature <= 38.5 else "üî¥"
            temp_note = "Normale" if 36.5 <= c.temperature <= 37.5 else "Fi√®vre" if c.temperature > 37.5 else "Hypothermie"
            st.metric("üå°Ô∏è Temp√©rature", f"{c.temperature}¬∞C", help=f"{temp_status} {temp_note}")
        
        with col3:
            # TA
            ta_status = "üü¢" if 100 <= c.ta_systolique <= 140 and 60 <= c.ta_diastolique <= 90 else "üü°" if c.ta_systolique <= 160 else "üî¥"
            ta_note = "Normale" if 100 <= c.ta_systolique <= 140 else "√âlev√©e (HTA)" if c.ta_systolique > 140 else "Basse"
            st.metric("ü©∫ Tension Art√©rielle (TA)", f"{c.ta_systolique}/{c.ta_diastolique} mmHg", help=f"{ta_status} {ta_note}")
            
            # Info patient
            st.metric("üë§ Patient", f"{extr.age or orig.age} ans, {extr.sexe or orig.sexe}")
        
        # Message explicatif
        st.info(f"""
        üí° **Ces constantes sont g√©n√©r√©es automatiquement par l'IA** pour √™tre coh√©rentes avec la pathologie :  
        *"{result['pathology']}"*
        
        Elles seront utilis√©es par le mod√®le de Machine Learning pour pr√©dire le niveau de gravit√©.
        """)
    
    st.markdown("---")
    
    # Tabs
    tab1, tab2, tab3 = st.tabs(["üí¨ Conversation", "üìä Donn√©es Patient", "üíæ Export ML"])
    
    with tab1:
        st.markdown("### üí¨ Conversation compl√®te")
        
        # Afficher conversation (SANS les explications)
        for msg in result['conversation'].messages:
            content = msg.content
            
            # Nettoyer les explications (tout ce qui suit "Explication", "Explications")
            if '"' in content:
                # Prendre seulement ce qui est avant les explications
                parts = content.split('"')
                if len(parts) > 1:
                    content = parts[0] + '"'
            
            # Enlever les explications qui commencent par "Explication"
            if "Explication" in content:
                content = content.split("Explication")[0].strip()
            
            if msg.role.value == "user":
                st.markdown(f"**üë®‚Äç‚öïÔ∏è Infirmier :** {content}")
            else:
                st.markdown(f"**ü§í Patient :** {content}")
            st.markdown("")
    
    with tab2:
        st.markdown("### üìä Informations extraites de la conversation")
        
        col_a, col_b = st.columns(2)
        
        with col_a:
            extr = result['extracted_patient']
            
            st.markdown("**üë§ Identit√©**")
            st.write(f"- √Çge : {extr.age} ans")
            st.write(f"- Sexe : {extr.sexe}")
            
            st.markdown("**ü©∫ Sympt√¥mes d√©clar√©s**")
            if extr.symptomes_exprimes:
                for s in extr.symptomes_exprimes:
                    st.write(f"- {s}")
            else:
                st.write("Aucun sympt√¥me extrait")
            
            if extr.duree_symptomes:
                st.markdown(f"**‚è±Ô∏è Dur√©e :** {extr.duree_symptomes}")
        
        with col_b:
            st.markdown("**üè• Ant√©c√©dents m√©dicaux**")
            if extr.antecedents:
                for a in extr.antecedents:
                    st.write(f"- {a}")
            else:
                st.write("Aucun ant√©c√©dent d√©clar√©")
            
            # Compl√©tude
            completeness = result['completeness']
            st.markdown(f"**‚úÖ Compl√©tude de l'information**")
            st.progress(completeness['score'])
            st.write(f"{completeness['score']*100:.0f}% des informations collect√©es")
            
            if completeness['missing']:
                st.warning(f"Manquant : {', '.join(completeness['missing'])}")
        
        st.info("üí° Les **constantes vitales** (FC, SpO2, TA, etc.) sont affich√©es dans la section ci-dessus et proviennent de la g√©n√©ration automatique coh√©rente avec la pathologie.")
    
    with tab3:
        st.markdown("### üíæ Donn√©es pour Machine Learning")
        
        # R√©cup√©rer les donn√©es ML
        llm = LLMFactory.create("mistral", "mistral-large-latest")
        workflow = SimulationWorkflow(llm)
        workflow.original_patient = result['original_patient']
        workflow.extracted_patient = result['extracted_patient']
        workflow.pathology = result['pathology']
        workflow.conversation = result['conversation']
        
        ml_data = workflow.export_for_ml()
        
        st.json(ml_data)
        
        # Bouton t√©l√©charger
        json_str = json.dumps(ml_data, indent=2, ensure_ascii=False)
        st.download_button(
            label="üì• T√©l√©charger (JSON)",
            data=json_str,
            file_name="conversation_triage.json",
            mime="application/json"
        )
    
    # ‚≠ê NOUVELLE SECTION : Pr√©diction ML
    st.markdown("---")
    st.subheader("ü§ñ Pr√©diction de Gravit√©")
    
    if st.button("üîÆ Pr√©dire le niveau de gravit√©", type="primary"):
        # Charger le mod√®le
        import pickle
        import numpy as np
        import time
        
        model_path = root_path / "src" / "models" / "random_forest_simple.pkl"
        
        if not model_path.exists():
            st.error(f"‚ùå Mod√®le non trouv√© ")
            
        else:
            with st.spinner("Chargement du mod√®le et pr√©diction..."):
                start_time = time.time()
                
                # Charger mod√®le
                with open(model_path, 'rb') as f:
                    clf = pickle.load(f)
                
                # R√©cup√©rer donn√©es ML
                llm = LLMFactory.create("mistral", "mistral-large-latest")
                workflow = SimulationWorkflow(llm)
                workflow.original_patient = result['original_patient']
                workflow.extracted_patient = result['extracted_patient']
                workflow.pathology = result['pathology']
                workflow.conversation = result['conversation']
                
                ml_data = workflow.export_for_ml()
                
                # Extraire constantes
                fc = ml_data.get('fc', 80)
                fr = ml_data.get('fr', 16)
                spo2 = ml_data.get('spo2', 98)
                ta_sys = ml_data.get('ta_systolique', 120)
                ta_dia = ml_data.get('ta_diastolique', 80)
                temp = ml_data.get('temperature', 37.0)
                age = ml_data.get('age', 50)
                sexe = ml_data.get('sexe', 'M')
                
                # Normaliser
                fc_norm = (fc - 70) / 30
                fr_norm = (fr - 16) / 5
                spo2_norm = (spo2 - 95) / 5
                ta_sys_norm = (ta_sys - 120) / 20
                ta_dia_norm = (ta_dia - 80) / 10
                temp_norm = (temp - 37) / 2
                age_norm = (age - 50) / 25
                sexe_encoded = 1 if sexe == 'M' else 0
                
                # Features
                features = np.array([[fc_norm, fr_norm, spo2_norm, ta_sys_norm, ta_dia_norm, temp_norm, age_norm, sexe_encoded]])
                
                # Pr√©diction
                prediction = clf.predict(features)[0]
                probas = clf.predict_proba(features)[0]
                proba_dict = dict(zip(clf.classes_, probas))
                
                duration = time.time() - start_time
                
                # Track pr√©diction ML
                try:
                    sys.path.insert(0, str(root_path / "src"))
                    from src.monitoring.metrics_tracker import get_tracker
                    
                    tracker = get_tracker()
                    
                    # Track pr√©diction
                    tracker.track_prediction(
                        severity=prediction,
                        age=age,
                        sex=sexe[0],
                        symptoms=[],
                        red_flags=[],
                        confidence=max(probas)
                    )
                    
                    # Track latence
                    tracker.track_latency(
                        component="Predictor_ML",
                        operation="predict",
                        duration=duration
                    )
                except:
                    pass
                
                # Affichage r√©sultat
                colors = {
                    "ROUGE": "üî¥",
                    "JAUNE": "üü°",
                    "VERT": "üü¢",
                    "GRIS": "‚ö™"
                }
                
                descriptions = {
                    "ROUGE": "**Urgence vitale imm√©diate** - Pronostic vital engag√©",
                    "JAUNE": "**Urgent mais non vital** - Prise en charge rapide n√©cessaire",
                    "VERT": "**Non urgent** - Peut attendre",
                    "GRIS": "**Ne n√©cessite pas les urgences**"
                }
                
                # R√©sultat principal
                st.success(f"### {colors[prediction]} Gravit√© pr√©dite : **{prediction}**")
                st.info(descriptions[prediction])
                
                # Probabilit√©s
                st.markdown("#### üìä Probabilit√©s par classe")
                
                col1, col2, col3, col4 = st.columns(4)
                
                for col, label in zip([col1, col2, col3, col4], ["ROUGE", "JAUNE", "VERT", "GRIS"]):
                    with col:
                        prob = proba_dict.get(label, 0)
                        st.metric(
                            label=f"{colors[label]} {label}",
                            value=f"{prob*100:.1f}%"
                        )
                        st.progress(prob)
                
                # D√©tails
                with st.expander("üìã D√©tails de la pr√©diction"):
                    st.markdown("**Constantes utilis√©es pour la pr√©diction :**")
                    col_a, col_b, col_c = st.columns(3)
                    
                    with col_a:
                        st.write(f"- FC : {fc} bpm")
                        st.write(f"- FR : {fr} /min")
                        st.write(f"- SpO2 : {spo2}%")
                    
                    with col_b:
                        st.write(f"- TA : {ta_sys}/{ta_dia} mmHg")
                        st.write(f"- Temp : {temp}¬∞C")
                    
                    with col_c:
                        st.write(f"- √Çge : {age} ans")
                        st.write(f"- Sexe : {sexe}")
                    
                    st.markdown("**Features normalis√©es :**")
                    st.code(f"[{fc_norm:.3f}, {fr_norm:.3f}, {spo2_norm:.3f}, {ta_sys_norm:.3f}, {ta_dia_norm:.3f}, {temp_norm:.3f}, {age_norm:.3f}, {sexe_encoded}]")

# Dataset complet
st.markdown("---")
st.subheader("üìä Dataset complet")

if st.session_state.conversations:
    st.info(f"**{len(st.session_state.conversations)} conversations** g√©n√©r√©es")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # T√©l√©charger JSON
        dataset_json = json.dumps(st.session_state.conversations, indent=2, ensure_ascii=False)
        st.download_button(
            label="üì• T√©l√©charger dataset (JSON)",
            data=dataset_json,
            file_name="dataset_triage.json",
            mime="application/json"
        )
    
    with col2:
        # T√©l√©charger CSV
        import pandas as pd
        df = pd.DataFrame(st.session_state.conversations)
        csv = df.to_csv(index=False, encoding='utf-8')
        st.download_button(
            label="üì• T√©l√©charger dataset (CSV)",
            data=csv,
            file_name="dataset_triage.csv",
            mime="text/csv"
        )
    
    with col3:
        # Reset
        if st.button("üóëÔ∏è Effacer le dataset"):
            st.session_state.conversations = []
            st.session_state.current_result = None
            st.rerun()
    
    # Aper√ßu du dataset
    with st.expander("üëÅÔ∏è Aper√ßu du dataset"):
        import pandas as pd
        df = pd.DataFrame(st.session_state.conversations)
        st.dataframe(df, use_container_width=True)

else:
    st.info("Aucune conversation g√©n√©r√©e. Cliquez sur 'G√©n√©rer' pour commencer !")